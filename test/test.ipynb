{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Type\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DiskCapacityInput(BaseModel):\n",
    "    host: str = Field(..., description=\"Hostname or IP of the server\")\n",
    "    user: str = Field(..., description=\"SSH username\")\n",
    "    ssh_key_path: str = Field(..., description=\"Path to private SSH key\")\n",
    "\n",
    "class DiskCapacity(BaseTool):\n",
    "    name: str = \"Check disk capacity\"\n",
    "    description: str = \"\"\"\n",
    "Connects via SSH to a remote Linux server and checks the disk usage of the root (/) filesystem.\n",
    "Required arguments:\n",
    "- host: The hostname or IP address of the remote server.\n",
    "- user: The SSH username used for authentication.\n",
    "- ssh_key_path: The local file path to the private SSH key used for connecting.\n",
    "\n",
    "Returns the output of the 'df -h /' command, or an error message if the connection fails.\n",
    "\"\"\"\n",
    "    args_schema: Type[DiskCapacityInput] = DiskCapacityInput\n",
    "    def _run(\n",
    "        self,\n",
    "        host: str,\n",
    "        user: str,\n",
    "        ssh_key_path: str\n",
    "    ) -> str:\n",
    "        import subprocess\n",
    "        \n",
    "        print(f\"Connecting to {host} with user {user} and key {ssh_key_path}\")\n",
    "\n",
    "        cmd = [\n",
    "            \"ssh\",\n",
    "            \"-i\", ssh_key_path,\n",
    "            \"-o\", \"StrictHostKeyChecking=no\",\n",
    "            f\"{user}@{host}\",\n",
    "            \"df -h\"\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        print(result)\n",
    "        if result.returncode != 0:\n",
    "            return f\"Error: {result.stderr.strip()}\"\n",
    "        return result.stdout.strip()\n",
    "\n",
    "\n",
    "test = DiskCapacity()\n",
    "rslt = test._run(host=\"node-0\", user=\"root\", ssh_key_path=\"~/.ssh/id_rsa\")\n",
    "rslt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b051947",
   "metadata": {},
   "source": [
    "# prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf242f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_api_client import PrometheusConnect\n",
    "import json\n",
    "\n",
    "def classify_target(target):\n",
    "    # K8s basic components keywords based on the provided targets\n",
    "    k8s_basic_keywords = ['apiserver', 'coredns', 'kube-controller-manager', 'kube-proxy', 'kube-scheduler', 'kubelet']\n",
    "    # Observability associated components keywords based on the provided targets (Prometheus, Alertmanager, etc.)\n",
    "    observability_keywords = ['alertmanager', 'prometheus', 'kube-state-metrics', 'node-exporter', 'operator']\n",
    "\n",
    "    # Use 'job' or 'container' to identify the component\n",
    "    job_name = target.get('job', '').lower()\n",
    "    container_name = target.get('container', '').lower() if target.get('container') else ''\n",
    "\n",
    "    # Check for K8s basic components\n",
    "    for keyword in k8s_basic_keywords:\n",
    "        if keyword in job_name or keyword in container_name:\n",
    "            return 'K8sComponents'\n",
    "\n",
    "    # Check for observability associated components\n",
    "    for keyword in observability_keywords:\n",
    "        if keyword in job_name or keyword in container_name:\n",
    "            return 'ObservabilityComponents'\n",
    "\n",
    "    # If not classified, categorize as 'others'\n",
    "    return 'others'\n",
    "\n",
    "def get_prometheus_targets(prometheus_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Queries the Prometheus API to retrieve all configured targets and their status.\n",
    "\n",
    "    Args:\n",
    "        prometheus_url: The URL of the Prometheus server (e.g., \"http://localhost:9090\").\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the active and dropped targets, as retrieved from the API.\n",
    "    \"\"\"\n",
    "    # Initialize the PrometheusConnect client.\n",
    "    # This establishes a connection object pointing to the Prometheus server URL.\n",
    "    prom = PrometheusConnect(url=prometheus_url)\n",
    "\n",
    "    try:\n",
    "        # Use the get_targets() method to fetch target information from the\n",
    "        # Prometheus /api/v1/targets endpoint.\n",
    "        targets_info = prom.get_targets()\n",
    "        # print(json.dumps(targets_info, indent=2))  # Debugging output\n",
    "\n",
    "        # The API response typically contains 'activeTargets' and 'droppedTargets'.\n",
    "        # We return the entire dictionary for comprehensive information.\n",
    "\n",
    "        lstRtn = list()\n",
    "        lstTagets = targets_info.get(\"activeTargets\")\n",
    "        for taget in lstTagets:\n",
    "            lstRtn.append({\n",
    "                \"job\": taget.get(\"labels\").get(\"job\"),\n",
    "                \"container\": taget.get(\"labels\").get(\"container\"),\n",
    "                \"namespace\": taget.get(\"labels\").get(\"namespace\"),\n",
    "                \"service\": taget.get(\"labels\").get(\"service\"),\n",
    "                \"pod\": taget.get(\"labels\").get(\"pod\"),\n",
    "                \"health\": taget.get(\"health\"),\n",
    "                \"lastScrape\": taget.get(\"lastScrape\")\n",
    "            })\n",
    "        del targets_info\n",
    "        \n",
    "        # Classify the targets\n",
    "        result = {\n",
    "            'K8sComponents': [],\n",
    "            'ObservabilityComponents': [],\n",
    "            'others': []\n",
    "        }\n",
    "\n",
    "        for target in lstRtn:\n",
    "            category = classify_target(target)\n",
    "            result[category].append(target)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle potential connection errors or API issues.\n",
    "        print(f\"Error querying Prometheus targets: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example Usage:\n",
    "prometheus_server_url = \"http://192.168.72.59:9090\"\n",
    "targets_data = get_prometheus_targets(prometheus_server_url)\n",
    "print(targets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef12776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_api_client import PrometheusConnect\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "def query_target_metrics_timeframe(\n",
    "    prometheus_url: str, \n",
    "    promql_query: str, \n",
    "    start_time: datetime, \n",
    "    end_time: datetime, \n",
    "    step_size: str = \"30s\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Queries specific metrics from Prometheus over a defined timeframe.\n",
    "\n",
    "    Args:\n",
    "        prometheus_url: The URL of the Prometheus server (e.g., \"http://localhost:9090\").\n",
    "        promql_query: The PromQL expression to execute (e.g., 'node_cpu_seconds_total').\n",
    "        start_time: The start timestamp of the query range (datetime object).\n",
    "        end_time: The end timestamp of the query range (datetime object).\n",
    "        step_size: The resolution step width for the query (e.g., \"1m\", \"30s\").\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the query results in Prometheus matrix format.\n",
    "    \"\"\"\n",
    "    # Initialize the PrometheusConnect client.\n",
    "    prom = PrometheusConnect(url=prometheus_url)\n",
    "\n",
    "    try:\n",
    "        # The custom_query_range method executes a PromQL query over a specified range.\n",
    "        # It takes the query string, start time, end time, and step size.\n",
    "        # This is the standard approach for range queries (API endpoint: /api/v1/query_range).\n",
    "        query_results = prom.custom_query_range(\n",
    "            query=promql_query,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            step=step_size\n",
    "        )\n",
    "\n",
    "        # The results are returned as a list of dictionaries, representing time series data.\n",
    "        return query_results\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle potential errors during the query execution.\n",
    "        print(f\"Error executing range query: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36868f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage for CPU Utilization (assuming node_exporter metrics are available):\n",
    "\n",
    "# Define connection details and timeframe\n",
    "prometheus_server_url = \"http://192.168.72.59:9090\"\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(minutes=3) # Querying data from the last hour\n",
    "\n",
    "# Common PromQL query for CPU utilization (percentage of non-idle CPU usage)\n",
    "# Note: Metric names like 'node_cpu_seconds_total' are typically provided by node_exporter.\n",
    "# The 'instance=\"your_target_ip:port\"' label filters the data for a specific target.\n",
    "# memory: `100 * (1 - ((node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) / node_memory_MemTotal_bytes))`\n",
    "# network receive: `rate(node_network_receive_bytes_total[5m])`\n",
    "# network transmit: `rate(node_network_transmit_bytes_total[5m])`\n",
    "# IOPS: `rate(node_disk_reads_completed_total[5m]) + rate(node_disk_writes_completed_total[5m])`\n",
    "query = 'kube_pod_info'\n",
    "\n",
    "# Example execution (uncomment to run):\n",
    "query = query_target_metrics_timeframe(\n",
    "    prometheus_server_url, \n",
    "    query, \n",
    "    start_time, \n",
    "    end_time, \n",
    "    step_size=\"1m\"\n",
    ")\n",
    "# print(json.dumps(query, indent=4))\n",
    "\n",
    "lstTmp = []\n",
    "for i in query:\n",
    "    lstTmp.append({\n",
    "        \"namespace\": i.get('metric').get(\"namespace\"),\n",
    "        \"service\": i.get('metric').get(\"service\"),\n",
    "        \"pod\": i.get('metric').get(\"pod\"),\n",
    "        \"node\": i.get('metric').get(\"node\"),\n",
    "    })\n",
    "\n",
    "from collections import defaultdict\n",
    "# Group lstTmp by namespace\n",
    "grouped_by_namespace = defaultdict(list)\n",
    "for item in lstTmp:\n",
    "    namespace = item.get(\"namespace\", \"unknown\")  # Use \"unknown\" if namespace is None\n",
    "    grouped_by_namespace[namespace].append(item)\n",
    "\n",
    "# Convert defaultdict to regular dict for cleaner output\n",
    "grouped_by_namespace = dict(grouped_by_namespace)\n",
    "print(json.dumps(grouped_by_namespace, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_server_url = \"http://192.168.72.59:9090\"\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(minutes=1) # Querying data from the last hour\n",
    "# pod name: controller-58fdf44d87-tmtqt\n",
    "# CPU, average CPU utilization of the entire pod in **CPU core per second**\n",
    "# query = 'sum(rate(container_cpu_usage_seconds_total{pod=\"controller-58fdf44d87-tmtqt\", container!=\"\"}[5m])) by (pod)'\n",
    "# MEM\n",
    "# query = 'sum(rate(container_memory_working_set_bytes{pod=\"controller-58fdf44d87-tmtqt\", container!=\"\"}[5m])) by (pod)'\n",
    "# Network  \n",
    "# query = 'sum(rate(container_network_receive_bytes_total{pod!=\"\"}[5m])) by (pod)'\n",
    "# query = 'sum(rate(container_network_transmit_bytes_total{pod!=\"\"}[5m])) by (pod, namespace)'\n",
    "# pod status\n",
    "# query = 'kube_pod_status_phase{pod=\"controller-58fdf44d87-tmtqt\", phase=\"Running\"}'\n",
    "\n",
    "query_conditions = {'cpu':'sum(rate(container_cpu_usage_seconds_total{pod=\"***\"}[1m])) by (pod)',\n",
    "                    'mem':'sum(rate(container_memory_working_set_bytes{pod=\"***\"}[1m])) by (pod)',\n",
    "                    'net_in':'sum(rate(container_network_receive_bytes_total{pod=\"***\"}[1m])) by (pod)',\n",
    "                    'net_out':'sum(rate(container_network_transmit_bytes_total{pod=\"***\"}[1m])) by (pod, namespace)',\n",
    "                    'pod_status':'kube_pod_status_phase{pod=\"***\", phase=\"Running\"}'\n",
    "                    }\n",
    "\n",
    "keys = grouped_by_namespace.keys()\n",
    "for ns in keys:\n",
    "    for pod_info in grouped_by_namespace.get(ns):\n",
    "        podname = pod_info.get('pod')\n",
    "        for key in query_conditions.keys():\n",
    "            # Example execution (uncomment to run):\n",
    "            rslt = query_target_metrics_timeframe(\n",
    "                prometheus_server_url, \n",
    "                query_conditions.get(key).replace(\"***\", podname),\n",
    "                start_time, \n",
    "                end_time\n",
    "            )\n",
    "            value = [float(sublist[-1]) for sublist in rslt[0].get('values')]\n",
    "            pod_info.update({key: value})\n",
    "\n",
    "grouped_by_namespace\n",
    "\n",
    "# rslt = query_target_metrics_timeframe(\n",
    "#             prometheus_server_url, \n",
    "#             query, \n",
    "#             start_time, \n",
    "#             end_time\n",
    "#         )\n",
    "# print(json.dumps(rslt, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe0cb4",
   "metadata": {},
   "source": [
    "# Loki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "LOKI_URL = \"http://192.168.72.58:3100\" # From kubectl get service output\n",
    "\n",
    "def get_loki_labels_api():\n",
    "    \"\"\"\n",
    "    Fetches available label names from Loki.\n",
    "    These labels represent the 'targets' or streams of logs.\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is a label name.\n",
    "        dict: Error details if any.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{LOKI_URL}/loki/api/v1/labels\")\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data['status'] == 'success':\n",
    "            return data['data'], None\n",
    "        else:\n",
    "            return [], {\"error\": data.get('error', 'Unknown error'), \"status\": data['status']}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return [], {\"error\": f\"Could not connect to Loki at {LOKI_URL}. Please ensure Loki is running and accessible.\", \"status\": \"connection_error\"}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return [], {\"error\": f\"An HTTP request error occurred: {e}\", \"status\": \"http_error\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return [], {\"error\": \"Failed to decode JSON response from Loki.\", \"status\": \"json_decode_error\"}\n",
    "    except Exception as e:\n",
    "        return [], {\"error\": f\"An unexpected error occurred: {e}\", \"status\": \"unknown_error\"}\n",
    "\n",
    "def get_loki_label_values_api(label_name: str):\n",
    "    \"\"\"\n",
    "    Fetches values for a specific label from Loki.\n",
    "    Args:\n",
    "        label_name (str): The name of the label (e.g., 'app', 'namespace').\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is a value for the given label.\n",
    "        dict: Error details if any.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{LOKI_URL}/loki/api/v1/label/{label_name}/values\")\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data['status'] == 'success':\n",
    "            return data['data'], None\n",
    "        else:\n",
    "            return [], {\"error\": data.get('error', 'Unknown error'), \"status\": data['status']}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return [], {\"error\": f\"Could not connect to Loki at {LOKI_URL}. Please ensure Loki is running and accessible.\", \"status\": \"connection_error\"}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return [], {\"error\": f\"An HTTP request error occurred: {e}\", \"status\": \"http_error\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return [], {\"error\": \"Failed to decode JSON response from Loki.\", \"status\": \"json_decode_error\"}\n",
    "    except Exception as e:\n",
    "        return [], {\"error\": f\"An unexpected error occurred: {e}\", \"status\": \"unknown_error\"}\n",
    "\n",
    "def query_loki_logs_api(query: str, start_time_str: str, end_time_str: str):\n",
    "    \"\"\"\n",
    "    Queries Loki for logs matching a LogQL query within a specified time range.\n",
    "    Args:\n",
    "        query (str): The LogQL query string.\n",
    "        start_time_str (str): Start time in 'YYYY-MM-DD HH:MM:SS' format.\n",
    "        end_time_str (str): End time in 'YYYY-MM-DD HH:MM:SS' format.\n",
    "        limit (int): The maximum number of log lines to retrieve. Default is 1000.\n",
    "    Returns:\n",
    "        list: A list of log entries, where each entry is a dictionary with 'stream' (labels) and 'values' (timestamps and log lines).\n",
    "        dict: Error details if any.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = datetime.datetime.strptime(start_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        end_time = datetime.datetime.strptime(end_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'start': int(start_time.timestamp() * 1e9), # Loki expects nanoseconds\n",
    "            'end': int(end_time.timestamp() * 1e9),     # Loki expects nanoseconds\n",
    "        }\n",
    "        response = requests.get(f\"{LOKI_URL}/loki/api/v1/query_range\", params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data['status'] == 'success':\n",
    "            # Format the output for easier consumption in an API\n",
    "            formatted_results = []\n",
    "            if data['data']['resultType'] == 'streams': # Ensure it's stream data\n",
    "                for result in data['data']['result']:\n",
    "                    stream_labels = result['stream']\n",
    "                    values = []\n",
    "                    for entry in result['values']:\n",
    "                        timestamp_ns = int(entry[0])\n",
    "                        log_line = entry[1]\n",
    "                        timestamp = datetime.datetime.fromtimestamp(timestamp_ns / 1e9).isoformat() # ISO 8601\n",
    "                        values.append({\"timestamp\": timestamp, \"log\": log_line})\n",
    "                    formatted_results.append({\"stream\": stream_labels, \"values\": values})\n",
    "            return formatted_results, None\n",
    "        else:\n",
    "            return [], {\"error\": data.get('error', 'Unknown error'), \"status\": data['status']}\n",
    "    except ValueError:\n",
    "        return [], {\"error\": \"Invalid date/time format or limit. Please use YYYY-MM-DD HH:MM:SS for time and an integer for limit.\", \"status\": \"invalid_input\"}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return [], {\"error\": f\"Could not connect to Loki at {LOKI_URL}. Please ensure Loki is running and accessible.\", \"status\": \"connection_error\"}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return [], {\"error\": f\"An HTTP request error occurred: {e}\", \"status\": \"http_error\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return [], {\"error\": \"Failed to decode JSON response from Loki.\", \"status\": \"json_decode_error\"}\n",
    "    except Exception as e:\n",
    "        return [], {\"error\": f\"An unexpected error occurred: {e}\", \"status\": \"unknown_error\"}\n",
    "\n",
    "# Example of how you would test these functions (without being a command-line program)\n",
    "print(\"\\n--- Testing Loki Functions ---\")\n",
    "\n",
    "# Test get_loki_labels_api\n",
    "labels, err = get_loki_labels_api()\n",
    "if err:\n",
    "    print(f\"Error fetching labels: {err}\")\n",
    "else:\n",
    "    print(\"\\nAvailable Loki Labels:\")\n",
    "    for label in labels[:]: # Print labels\n",
    "        print(f\"  - {label}\")\n",
    "print(labels)\n",
    "\n",
    "# Test get_loki_label_values_api\n",
    "label_value = dict()\n",
    "if labels:\n",
    "    for label in labels:\n",
    "        values, err = get_loki_label_values_api(label)\n",
    "        if err:\n",
    "            print(f\"Error fetching label values for '{label}': {err}\")\n",
    "        else:\n",
    "            label_value.update({label: values})\n",
    "print(label_value)\n",
    "\n",
    "\n",
    "### --- ###\n",
    "# Test query_loki_logs_api\n",
    "print(\"\\nQuerying Loki Logs:\")\n",
    "# Using current time as a reference for demonstration\n",
    "end_time_example = datetime.datetime.now()\n",
    "start_time_example = end_time_example - datetime.timedelta(minutes=30) # Last 30 minutes\n",
    "start_time_str_example = start_time_example.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "end_time_str_example = end_time_example.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Example: Query for logs from any stream (if any exist)\n",
    "# You might need to adjust this query based on what labels your Loki instance has.\n",
    "# A common starting point is to query based on a 'job' or 'namespace' label.\n",
    "# For example: '{job=\"kubernetes-pods\"}' or '{namespace=\"default\"}'\n",
    "# log_query_example = '{job=~\".+\"}' # This is a very broad query, might be slow/large\n",
    "log_query_example = '{app=\"calico-node\"}' # More specific query, adjust as needed\n",
    "print(f\"Attempting to query with LogQL: '{log_query_example}' for the last 30 minutes...\")\n",
    "logs, err = query_loki_logs_api(log_query_example, start_time_str_example, end_time_str_example)\n",
    "print(logs)\n",
    "if err:\n",
    "    print(f\"Error querying logs: {err}\")\n",
    "else:\n",
    "    if logs:\n",
    "        print(\"Found Logs (first 2 streams, 3 lines each):\")\n",
    "        for stream_group in logs:\n",
    "            print(f\"  Stream: {stream_group['stream']}\")\n",
    "            for log_entry in stream_group['values']:\n",
    "                print(f\"    [{log_entry['timestamp']}] {log_entry['log']}\")\n",
    "            if len(stream_group['values']) > 3:\n",
    "                print(f\"    ... and {len(stream_group['values']) - 3} more entries in this stream.\")\n",
    "        if len(logs) > 2:\n",
    "            print(f\"  ... and {len(logs) - 2} more log streams.\")\n",
    "    else:\n",
    "        print(\"No logs found for the given query and time range.\")\n",
    "\n",
    "# Example with a specific label (if you know one exists, e.g., 'app')\n",
    "# Replace 'some_app_name' with an actual application name from your Loki setup\n",
    "# log_query_specific = '{app=\"some_app_name\"}'\n",
    "# print(f\"\\nAttempting to query with LogQL: '{log_query_specific}'...\")\n",
    "# specific_logs, err_specific = query_loki_logs_api(log_query_specific, start_time_str_example, end_time_str_example, limit=5)\n",
    "# if err_specific:\n",
    "#     print(f\"Error querying specific logs: {err_specific}\")\n",
    "# else:\n",
    "#     if specific_logs:\n",
    "#         print(\"Found Specific Logs:\")\n",
    "#         # Process specific_logs similarly\n",
    "#     else:\n",
    "#         print(\"No specific logs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbdc94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-07-15 07:19:09'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "start_time_str = \"2023-10-01 00:00:00\"  # Example start time\n",
    "start_time = datetime.datetime.strptime(start_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "int(start_time.timestamp()) # unix timestamp in seconds\n",
    "date = datetime.datetime.now() - datetime.timedelta(minutes=30) # 30 minutes ago\n",
    "date.strftime(\"%Y-%m-%d %H:%M:%S\") # Format to string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prjtest-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
